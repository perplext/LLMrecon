"""
Unsupervised Vulnerability Discovery System for LLMrecon

This module implements unsupervised learning techniques to discover
new vulnerabilities and attack patterns in LLMs without labeled data.
"""

import numpy as np
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA, LatentDirichletAllocation
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import torch
import torch.nn as nn
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
import json
import logging
from collections import defaultdict, Counter
import re
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class VulnerabilityPattern:
    """Represents a discovered vulnerability pattern"""
    pattern_id: str
    pattern_type: str  # 'syntactic', 'semantic', 'behavioral'
    examples: List[str]
    features: np.ndarray
    confidence: float
    discovered_at: datetime
    metadata: Dict[str, Any]


class FeatureExtractor:
    """Extract features from attack payloads and responses"""
    
    def __init__(self):
        self.syntactic_patterns = [
            r'\[INST\]', r'\[/INST\]', r'<\|im_start\|>', r'<\|im_end\|>',
            r'###', r'```', r'System:', r'Assistant:', r'Human:',
            r'ignore.*previous', r'disregard.*instructions', r'act as',
            r'pretend', r'roleplay', r'simulate', r'bypass', r'override'
        ]
        
        self.semantic_markers = [
            'ethical', 'harmful', 'illegal', 'safety', 'guidelines',
            'restricted', 'cannot', 'unable', 'apologize', 'refuse'
        ]
    
    def extract_features(self, payload: str, response: str) -> np.ndarray:
        """Extract comprehensive features from payload and response"""
        features = []
        
        # Syntactic features
        features.extend(self._extract_syntactic_features(payload))
        
        # Semantic features
        features.extend(self._extract_semantic_features(payload, response))
        
        # Statistical features
        features.extend(self._extract_statistical_features(payload, response))
        
        # Response characteristics
        features.extend(self._extract_response_features(response))
        
        return np.array(features)
    
    def _extract_syntactic_features(self, text: str) -> List[float]:
        """Extract syntactic pattern features"""
        features = []
        
        # Pattern presence
        for pattern in self.syntactic_patterns:
            features.append(float(bool(re.search(pattern, text, re.IGNORECASE))))
        
        # Special character ratios
        features.append(len(re.findall(r'[<>\[\]{}]', text)) / max(len(text), 1))
        features.append(len(re.findall(r'[!?]', text)) / max(len(text), 1))
        
        # Uppercase ratio
        features.append(sum(1 for c in text if c.isupper()) / max(len(text), 1))
        
        # Repetition patterns
        words = text.lower().split()
        features.append(len(words) - len(set(words)) if words else 0)  # Repeated words
        
        return features
    
    def _extract_semantic_features(self, payload: str, response: str) -> List[float]:
        """Extract semantic features"""
        features = []
        
        # Semantic markers in response
        response_lower = response.lower()
        for marker in self.semantic_markers:
            features.append(float(marker in response_lower))
        
        # Payload-response similarity
        payload_words = set(payload.lower().split())
        response_words = set(response.lower().split())
        
        if payload_words and response_words:
            jaccard = len(payload_words & response_words) / len(payload_words | response_words)
            features.append(jaccard)
        else:
            features.append(0.0)
        
        # Command indicators
        command_words = ['tell', 'show', 'give', 'provide', 'explain', 'write']
        features.append(sum(1 for word in command_words if word in payload.lower()))
        
        return features
    
    def _extract_statistical_features(self, payload: str, response: str) -> List[float]:
        """Extract statistical features"""
        features = []
        
        # Length features
        features.append(len(payload))
        features.append(len(response))
        features.append(len(payload.split()))
        features.append(len(response.split()))
        
        # Entropy
        features.append(self._calculate_entropy(payload))
        features.append(self._calculate_entropy(response))
        
        # Response/payload ratio
        features.append(len(response) / max(len(payload), 1))
        
        return features
    
    def _extract_response_features(self, response: str) -> List[float]:
        """Extract features specific to response"""
        features = []
        
        # Refusal indicators
        refusal_phrases = ['cannot', 'unable', 'sorry', 'apologize', 'refuse', "can't", "won't"]
        features.append(sum(1 for phrase in refusal_phrases if phrase in response.lower()))
        
        # Compliance indicators
        compliance_phrases = ['here is', 'here are', 'certainly', 'sure', 'of course']
        features.append(sum(1 for phrase in compliance_phrases if phrase in response.lower()))
        
        # Question marks (uncertainty)
        features.append(response.count('?'))
        
        # Exclamation marks (emphasis)
        features.append(response.count('!'))
        
        return features
    
    def _calculate_entropy(self, text: str) -> float:
        """Calculate Shannon entropy of text"""
        if not text:
            return 0.0
        
        char_freq = Counter(text)
        total = len(text)
        entropy = 0.0
        
        for count in char_freq.values():
            p = count / total
            if p > 0:
                entropy -= p * np.log2(p)
        
        return entropy


class AnomalyDetector:
    """Detect anomalous patterns that might indicate vulnerabilities"""
    
    def __init__(self, contamination: float = 0.1):
        self.contamination = contamination
        self.scaler = StandardScaler()
        self.models = {}
    
    def fit(self, features: np.ndarray):
        """Fit anomaly detection models"""
        # Scale features
        features_scaled = self.scaler.fit_transform(features)
        
        # DBSCAN for density-based anomaly detection
        self.models['dbscan'] = DBSCAN(eps=0.5, min_samples=5)
        dbscan_labels = self.models['dbscan'].fit_predict(features_scaled)
        
        # Isolation Forest for anomaly detection
        from sklearn.ensemble import IsolationForest
        self.models['isolation_forest'] = IsolationForest(
            contamination=self.contamination,
            random_state=42
        )
        self.models['isolation_forest'].fit(features_scaled)
        
        # Local Outlier Factor
        from sklearn.neighbors import LocalOutlierFactor
        self.models['lof'] = LocalOutlierFactor(
            contamination=self.contamination,
            novelty=True
        )
        self.models['lof'].fit(features_scaled)
        
        return dbscan_labels
    
    def predict_anomalies(self, features: np.ndarray) -> Dict[str, np.ndarray]:
        """Predict anomalies using multiple methods"""
        features_scaled = self.scaler.transform(features)
        
        predictions = {}
        
        # DBSCAN (outliers are -1)
        if 'dbscan' in self.models:
            dbscan_pred = self.models['dbscan'].fit_predict(features_scaled)
            predictions['dbscan'] = dbscan_pred == -1
        
        # Isolation Forest (outliers are -1)
        if 'isolation_forest' in self.models:
            iso_pred = self.models['isolation_forest'].predict(features_scaled)
            predictions['isolation_forest'] = iso_pred == -1
        
        # LOF (outliers are -1)
        if 'lof' in self.models:
            lof_pred = self.models['lof'].predict(features_scaled)
            predictions['lof'] = lof_pred == -1
        
        # Ensemble prediction
        ensemble = np.zeros(len(features), dtype=bool)
        for pred in predictions.values():
            ensemble |= pred
        predictions['ensemble'] = ensemble
        
        return predictions


class PatternClustering:
    """Cluster attack patterns to discover vulnerability classes"""
    
    def __init__(self, n_clusters: int = 10):
        self.n_clusters = n_clusters
        self.kmeans = None
        self.pca = PCA(n_components=2)
        self.cluster_centers = None
    
    def fit_clusters(self, features: np.ndarray) -> np.ndarray:
        """Fit clustering model"""
        # Reduce dimensionality for visualization
        features_2d = self.pca.fit_transform(features)
        
        # Find optimal number of clusters using elbow method
        if len(features) > self.n_clusters:
            scores = []
            K = range(2, min(self.n_clusters + 1, len(features)))
            
            for k in K:
                kmeans = KMeans(n_clusters=k, random_state=42)
                labels = kmeans.fit_predict(features)
                score = silhouette_score(features, labels)
                scores.append(score)
            
            # Use best k
            best_k = K[np.argmax(scores)]
            self.n_clusters = best_k
        
        # Final clustering
        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)
        labels = self.kmeans.fit_predict(features)
        self.cluster_centers = self.kmeans.cluster_centers_
        
        return labels
    
    def get_cluster_characteristics(self, 
                                  features: np.ndarray, 
                                  labels: np.ndarray,
                                  payloads: List[str]) -> Dict[int, Dict[str, Any]]:
        """Analyze characteristics of each cluster"""
        cluster_info = {}
        
        for cluster_id in range(self.n_clusters):
            mask = labels == cluster_id
            cluster_features = features[mask]
            cluster_payloads = [p for i, p in enumerate(payloads) if mask[i]]
            
            if len(cluster_features) == 0:
                continue
            
            # Calculate cluster statistics
            center = self.cluster_centers[cluster_id]
            avg_distance = np.mean([
                np.linalg.norm(f - center) for f in cluster_features
            ])
            
            # Find most common patterns
            all_words = ' '.join(cluster_payloads).lower().split()
            word_freq = Counter(all_words)
            common_words = word_freq.most_common(10)
            
            # Identify cluster type based on features
            cluster_type = self._identify_cluster_type(center)
            
            cluster_info[cluster_id] = {
                'size': len(cluster_features),
                'center': center.tolist(),
                'avg_distance': avg_distance,
                'common_words': common_words,
                'type': cluster_type,
                'example_payloads': cluster_payloads[:5]
            }
        
        return cluster_info
    
    def _identify_cluster_type(self, center: np.ndarray) -> str:
        """Identify cluster type based on feature vector"""
        # This is a simplified heuristic - enhance based on actual features
        feature_names = [
            'instruction_manipulation', 'role_playing', 'encoding_exploit',
            'context_injection', 'semantic_attack', 'syntactic_anomaly'
        ]
        
        # Find dominant feature
        if len(center) >= len(feature_names):
            dominant_idx = np.argmax(center[:len(feature_names)])
            return feature_names[dominant_idx]
        
        return 'unknown'


class VulnerabilityDiscoverySystem:
    """
    Main system for discovering vulnerabilities using unsupervised learning.
    
    Features:
    - Anomaly detection for finding unusual patterns
    - Clustering for grouping similar vulnerabilities
    - Pattern evolution tracking
    - Automatic vulnerability classification
    """
    
    def __init__(self):
        self.feature_extractor = FeatureExtractor()
        self.anomaly_detector = AnomalyDetector()
        self.pattern_clustering = PatternClustering()
        self.discovered_patterns: List[VulnerabilityPattern] = []
        self.pattern_evolution = defaultdict(list)
    
    def analyze_attacks(self, 
                       attack_data: List[Dict[str, Any]],
                       min_confidence: float = 0.7) -> List[VulnerabilityPattern]:
        """
        Analyze attack data to discover vulnerability patterns.
        
        Args:
            attack_data: List of dicts with 'payload', 'response', 'success', etc.
            min_confidence: Minimum confidence for pattern discovery
            
        Returns:
            List of discovered vulnerability patterns
        """
        logger.info(f"Analyzing {len(attack_data)} attacks for vulnerability patterns")
        
        # Extract features
        features = []
        payloads = []
        responses = []
        
        for attack in attack_data:
            payload = attack['payload']
            response = attack['response']
            
            feature_vec = self.feature_extractor.extract_features(payload, response)
            features.append(feature_vec)
            payloads.append(payload)
            responses.append(response)
        
        features = np.array(features)
        
        # Detect anomalies
        anomaly_predictions = self.anomaly_detector.predict_anomalies(features)
        anomalies = anomaly_predictions['ensemble']
        
        # Cluster patterns
        cluster_labels = self.pattern_clustering.fit_clusters(features)
        cluster_info = self.pattern_clustering.get_cluster_characteristics(
            features, cluster_labels, payloads
        )
        
        # Discover patterns
        patterns = []
        
        # Anomaly-based patterns
        anomaly_indices = np.where(anomalies)[0]
        if len(anomaly_indices) > 0:
            anomaly_features = features[anomaly_indices]
            anomaly_payloads = [payloads[i] for i in anomaly_indices]
            
            pattern = VulnerabilityPattern(
                pattern_id=f"anomaly_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                pattern_type="behavioral",
                examples=anomaly_payloads[:10],
                features=np.mean(anomaly_features, axis=0),
                confidence=0.8,
                discovered_at=datetime.now(),
                metadata={
                    'discovery_method': 'anomaly_detection',
                    'anomaly_count': len(anomaly_indices)
                }
            )
            patterns.append(pattern)
        
        # Cluster-based patterns
        for cluster_id, info in cluster_info.items():
            if info['size'] >= 5:  # Minimum cluster size
                confidence = 1.0 - info['avg_distance'] / 10.0  # Normalize
                
                if confidence >= min_confidence:
                    pattern = VulnerabilityPattern(
                        pattern_id=f"cluster_{cluster_id}_{datetime.now().strftime('%Y%m%d')}",
                        pattern_type=info['type'],
                        examples=info['example_payloads'],
                        features=np.array(info['center']),
                        confidence=confidence,
                        discovered_at=datetime.now(),
                        metadata={
                            'discovery_method': 'clustering',
                            'cluster_size': info['size'],
                            'common_words': info['common_words']
                        }
                    )
                    patterns.append(pattern)
        
        # Topic modeling for semantic patterns
        semantic_patterns = self._discover_semantic_patterns(payloads, responses)
        patterns.extend(semantic_patterns)
        
        # Update discovered patterns
        self.discovered_patterns.extend(patterns)
        
        # Track pattern evolution
        for pattern in patterns:
            self.pattern_evolution[pattern.pattern_type].append({
                'timestamp': pattern.discovered_at,
                'confidence': pattern.confidence,
                'examples_count': len(pattern.examples)
            })
        
        logger.info(f"Discovered {len(patterns)} vulnerability patterns")
        return patterns
    
    def _discover_semantic_patterns(self, 
                                  payloads: List[str], 
                                  responses: List[str]) -> List[VulnerabilityPattern]:
        """Discover semantic patterns using topic modeling"""
        patterns = []
        
        # Prepare documents
        documents = [f"{p} {r}" for p, r in zip(payloads, responses)]
        
        # Simple tokenization
        from sklearn.feature_extraction.text import CountVectorizer
        vectorizer = CountVectorizer(max_features=100, stop_words='english')
        doc_term_matrix = vectorizer.fit_transform(documents)
        
        # LDA topic modeling
        n_topics = min(5, len(documents) // 10)
        if n_topics > 1:
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42
            )
            lda.fit(doc_term_matrix)
            
            # Extract topics
            feature_names = vectorizer.get_feature_names_out()
            
            for topic_idx, topic in enumerate(lda.components_):
                top_indices = topic.argsort()[-10:][::-1]
                top_words = [feature_names[i] for i in top_indices]
                
                # Find documents most associated with this topic
                doc_topic = lda.transform(doc_term_matrix)
                top_doc_indices = doc_topic[:, topic_idx].argsort()[-5:][::-1]
                
                pattern = VulnerabilityPattern(
                    pattern_id=f"semantic_{topic_idx}_{datetime.now().strftime('%Y%m%d')}",
                    pattern_type="semantic",
                    examples=[payloads[i] for i in top_doc_indices],
                    features=topic,
                    confidence=0.7,
                    discovered_at=datetime.now(),
                    metadata={
                        'discovery_method': 'topic_modeling',
                        'top_words': top_words,
                        'topic_weight': float(np.max(topic))
                    }
                )
                patterns.append(pattern)
        
        return patterns
    
    def find_similar_vulnerabilities(self, 
                                   query_payload: str,
                                   query_response: str,
                                   top_k: int = 5) -> List[VulnerabilityPattern]:
        """Find similar vulnerability patterns to a query"""
        # Extract features from query
        query_features = self.feature_extractor.extract_features(query_payload, query_response)
        
        # Calculate distances to all patterns
        distances = []
        for pattern in self.discovered_patterns:
            dist = np.linalg.norm(query_features - pattern.features)
            distances.append((dist, pattern))
        
        # Sort by distance
        distances.sort(key=lambda x: x[0])
        
        # Return top k
        return [pattern for _, pattern in distances[:top_k]]
    
    def generate_vulnerability_report(self) -> Dict[str, Any]:
        """Generate comprehensive vulnerability discovery report"""
        report = {
            'total_patterns_discovered': len(self.discovered_patterns),
            'pattern_types': {},
            'confidence_distribution': {},
            'temporal_analysis': {},
            'top_patterns': []
        }
        
        # Pattern type distribution
        type_counts = Counter(p.pattern_type for p in self.discovered_patterns)
        report['pattern_types'] = dict(type_counts)
        
        # Confidence distribution
        confidence_bins = {'high': 0, 'medium': 0, 'low': 0}
        for pattern in self.discovered_patterns:
            if pattern.confidence >= 0.8:
                confidence_bins['high'] += 1
            elif pattern.confidence >= 0.6:
                confidence_bins['medium'] += 1
            else:
                confidence_bins['low'] += 1
        report['confidence_distribution'] = confidence_bins
        
        # Temporal analysis
        for pattern_type, evolution in self.pattern_evolution.items():
            if evolution:
                report['temporal_analysis'][pattern_type] = {
                    'discoveries': len(evolution),
                    'avg_confidence': np.mean([e['confidence'] for e in evolution]),
                    'trend': 'increasing' if len(evolution) > 1 and evolution[-1]['confidence'] > evolution[0]['confidence'] else 'stable'
                }
        
        # Top patterns by confidence
        top_patterns = sorted(self.discovered_patterns, key=lambda p: p.confidence, reverse=True)[:5]
        report['top_patterns'] = [
            {
                'id': p.pattern_id,
                'type': p.pattern_type,
                'confidence': p.confidence,
                'examples': len(p.examples),
                'metadata': p.metadata
            }
            for p in top_patterns
        ]
        
        return report
    
    def export_patterns(self, filepath: str):
        """Export discovered patterns"""
        export_data = {
            'patterns': [
                {
                    'pattern_id': p.pattern_id,
                    'pattern_type': p.pattern_type,
                    'examples': p.examples,
                    'features': p.features.tolist(),
                    'confidence': p.confidence,
                    'discovered_at': p.discovered_at.isoformat(),
                    'metadata': p.metadata
                }
                for p in self.discovered_patterns
            ],
            'evolution': dict(self.pattern_evolution),
            'report': self.generate_vulnerability_report()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        logger.info(f"Exported {len(self.discovered_patterns)} patterns to {filepath}")