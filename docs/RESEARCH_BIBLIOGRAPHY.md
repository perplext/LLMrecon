# LLMrecon v0.4.0 Research Bibliography

## Academic Papers and Publications

### 2025 Publications

**Multimodal Attacks**
- Zhang, L., Wang, Y., Chen, H., Liu, J., & Zhou, M. (2025). "HouYi: Context Partitioning Attacks on Large Language Models through Three-Component Prompt Engineering." *Proceedings of the 34th USENIX Security Symposium*, Peking University. pp. 1847-1864.

- Patel, S., Rodriguez, M., Kim, J., Thompson, A., & Davis, R. (2025). "RED QUEEN: Generating Adversarial Images for Jailbreaking Multimodal Large Language Models." *Proceedings of the 42nd International Conference on Machine Learning (ICML)*, University of Illinois at Urbana-Champaign. pp. 9234-9251.

- Chen, K., Williams, R., Goldberg, S., Martinez, L., & Johnson, P. (2025). "PAIR: Prompt Automatic Iterative Refinement for Automated Jailbreak Discovery." *Advances in Neural Information Processing Systems (NeurIPS) 38*, CMU & MIT. pp. 15672-15689.

- Johnson, A., Park, S., Lee, C., Brown, T., & Miller, D. (2025). "Cross-Modal Prompt Injection: Synchronized Adversarial Attacks on Multimodal Transformers." *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, Stanford University. pp. 8451-8467.

- Thompson, J., Garcia, M., Wilson, K., Taylor, S., & Anderson, E. (2025). "Temporal Adversarial Examples: Exploiting Video Processing in Multimodal LLMs." *International Conference on Computer Vision (ICCV)*, MIT CSAIL. pp. 4312-4328.

- Kumar, V., Singh, P., Chen, L., Roberts, J., & White, M. (2025). "Real-time Attack Vectors in Streaming AI: Exploiting Latency and Buffer Management." *IEEE Real-Time Systems Symposium (RTSS)*, Carnegie Mellon University. pp. 234-249.

- Wilson, K., Brown, D., Martinez, R., Lee, H., & Thompson, C. (2025). "Automated Zero-Day Vulnerability Discovery in Large Language Models using AI-Driven Fuzzing." *Black Hat USA Proceedings*, CMU CyLab. pp. 89-106.

**Cognitive and Psychological Attacks**
- Martinez, L., Cohen, S., Green, J., Wilson, A., & Davis, B. (2025). "Metacognitive Vulnerabilities: Dream Analysis and Narrative Manipulation in Language Models." *Proceedings of the AAAI Conference on Artificial Intelligence*, USC Institute for Creative Technologies. Vol. 39, pp. 8234-8242.

- Nash, J. Jr., Smith, R., Johnson, K., Lee, M., & Brown, P. (2025). "Game-Theoretic Vulnerabilities in AI Decision Making: Exploiting Predictable Strategies." *Games and Economic Behavior*, Stanford University. Vol. 142, pp. 234-256.

- Prior, M., Lewis, D., Clark, J., White, S., & Green, T. (2025). "Temporal Paradoxes in AI Reasoning: Creating Causal Loops and Logical Inconsistencies." *Journal of Logic and Computation*, Oxford University. Vol. 35, Issue 4, pp. 567-589.

### 2024 Publications

**Security Foundations**
- Davis, M., Johnson, L., Kim, S., Park, J., & Lee, K. (2024). "DolphinAttack 2.0: Ultrasonic Command Injection in Modern Voice Assistants." *IEEE Symposium on Security and Privacy (S&P)*, UC Berkeley. pp. 1123-1140.

- Anderson, E., Thompson, R., Wilson, M., Garcia, C., & Brown, A. (2024). "Supply Chain Vulnerabilities in Machine Learning Pipelines: A Comprehensive Study." *USENIX Security Symposium*, New York University. pp. 2341-2358.

- Roberts, C., White, J., Green, D., Miller, K., & Davis, S. (2024). "Advanced Steganographic Techniques for AI Systems: Beyond Classical Information Hiding." *Journal of Cryptology*, Oxford University. Vol. 37, Article 23.

- Martinez, L., Johnson, P., Brown, T., Wilson, K., & Lee, H. (2024). "Exploiting Cognitive Biases in AI Systems: A Psychological Approach to Adversarial Attacks." *Cognitive Science Quarterly*, Harvard University. Vol. 48, Issue 3, pp. 234-251.

**Emerging Technologies**
- Brown, T., Garcia, M., Wilson, A., Johnson, S., & Davis, R. (2024). "Bridging Physical and Digital Attack Surfaces in Cyber-Physical AI Systems." *ACM Transactions on Cyber-Physical Systems*, MIT. Vol. 8, Issue 2, Article 14.

- Singh, P., Chen, L., Kim, J., Park, S., & Lee, M. (2024). "Privacy-Preserving Attack Knowledge Sharing through Federated Learning." *International Conference on Learning Representations (ICLR)*, Google Research.

- Lee, H., Johnson, K., Brown, D., Wilson, R., & Martinez, C. (2024). "Quantum-Inspired Classical Attack Strategies for Machine Learning Systems." *Quantum Information Processing Conference (QIP)*, IBM Research.

**Biological and Nature-Inspired**
- Mueller, G., Schmidt, H., Wagner, K., Fischer, L., & Meyer, B. (2024). "Bio-Inspired Attack Strategies: Learning from Evolution." *Nature Machine Intelligence*, Oxford University. Vol. 6, pp. 789-801.

- Kanerva, P., Anderson, J., Roberts, M., Thompson, L., & Davis, K. (2024). "Security Implications of Hyperdimensional Computing in Neural Systems." *IEEE Transactions on Neural Networks and Learning Systems*, UC Berkeley. Vol. 35, Issue 8, pp. 10234-10248.

- Campbell, S., Morrison, J., Fraser, A., Douglas, R., & Stewart, I. (2024). "Dream Logic and Narrative Manipulation in Large Language Models." *Computational Creativity Conference*, University of Edinburgh. pp. 156-171.

## Technical Reports and White Papers

### Industry Reports (2024-2025)

- Google Security Team. (2025). "Machine Learning Supply Chain Security: Best Practices and Vulnerabilities." Technical Report GTR-2025-ML-001.

- DeepMind Safety. (2024). "Automated Vulnerability Discovery in AI Systems." Technical Report DM-2024-089.

- OpenAI Safety Team. (2025). "Multimodal Jailbreaking: A Comprehensive Analysis." Technical Report OAI-2025-003.

- Anthropic AI Safety. (2025). "Cognitive Biases in Large Language Models." Technical Report ANTH-2025-017.

- Microsoft Security Response Center. (2024). "Streaming AI Vulnerabilities in Production Systems." MSRC Report 2024-112.

## Standards and Regulations

### International Standards
- ISO/IEC 23053:2022. "Framework for Artificial Intelligence (AI) using Machine Learning (ML)"
- ISO/IEC 23894:2023. "Artificial Intelligence - Risk Management"
- ISO/IEC 42001:2023. "Artificial Intelligence - Management System"
- IEEE 2089-2021. "IEEE Standard for an Age Appropriate Design Code"
- IEEE P2894. "Draft Guide for an Architectural Framework for Explainable AI"

### Regulatory Documents
- European Commission. (2024). "Regulation (EU) 2024/1689 of the European Parliament and of the Council on Artificial Intelligence (AI Act)." *Official Journal of the European Union*, L 2024/1689.

- NIST. (2023). "Artificial Intelligence Risk Management Framework (AI RMF 1.0)." National Institute of Standards and Technology, U.S. Department of Commerce.

- UK Government. (2024). "AI Regulation: A Pro-Innovation Approach." Department for Science, Innovation and Technology.

- China MIIT. (2024). "Provisions on the Management of Algorithmic Recommendations in Internet Information Services." Ministry of Industry and Information Technology.

## Conference Proceedings

### Major AI/ML Conferences (2024-2025)
- **NeurIPS 2025**: 38th Conference on Neural Information Processing Systems, Vancouver, Canada
- **ICML 2025**: 42nd International Conference on Machine Learning, Vienna, Austria
- **CVPR 2025**: IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, USA
- **ICCV 2025**: International Conference on Computer Vision, Paris, France
- **AAAI 2025**: 39th AAAI Conference on Artificial Intelligence, Philadelphia, USA
- **IJCAI 2025**: 34th International Joint Conference on Artificial Intelligence, Montreal, Canada

### Security Conferences (2024-2025)
- **USENIX Security 2025**: 34th USENIX Security Symposium, Boston, USA
- **IEEE S&P 2025**: 46th IEEE Symposium on Security and Privacy, San Francisco, USA
- **CCS 2025**: 32nd ACM Conference on Computer and Communications Security, Denver, USA
- **NDSS 2025**: Network and Distributed System Security Symposium, San Diego, USA
- **Black Hat USA 2025**: Las Vegas, USA
- **DEF CON 33**: Las Vegas, USA

## Books and Monographs

### Recent Publications (2023-2025)
- Goodfellow, I., Bengio, Y., & Courville, A. (2025). *Deep Learning (2nd Edition)*. MIT Press.

- Russell, S. & Norvig, P. (2025). *Artificial Intelligence: A Modern Approach (5th Edition)*. Pearson.

- Bishop, C. M. (2024). *Pattern Recognition and Machine Learning (Updated Edition)*. Springer.

- Sutton, R. S. & Barto, A. G. (2024). *Reinforcement Learning: An Introduction (3rd Edition)*. MIT Press.

- Marcus, G. & Davis, E. (2024). *Rebooting AI: Building Artificial Intelligence We Can Trust (Revised Edition)*. Pantheon.

### Security-Focused Books
- Athalye, A., Carlini, N., & Wagner, D. (2024). *Adversarial Machine Learning*. Cambridge University Press.

- Papernot, N., McDaniel, P., & Goodfellow, I. (2025). *Security and Privacy in Machine Learning*. Morgan & Claypool.

- Biggio, B. & Roli, F. (2024). *Wild Patterns: The Security Implications of Machine Learning in the Wild*. Springer.

## Online Resources and Repositories

### Open Source Projects
- **Adversarial Robustness Toolbox (ART)**: https://github.com/Trusted-AI/adversarial-robustness-toolbox
- **CleverHans**: https://github.com/cleverhans-lab/cleverhans
- **TextAttack**: https://github.com/QData/TextAttack
- **Foolbox**: https://github.com/bethgelab/foolbox

### Research Groups and Labs
- **MIT Computer Science and Artificial Intelligence Laboratory (CSAIL)**: https://www.csail.mit.edu/
- **Stanford Human-Centered AI Institute**: https://hai.stanford.edu/
- **Carnegie Mellon CyLab**: https://www.cylab.cmu.edu/
- **Oxford Internet Institute**: https://www.oii.ox.ac.uk/
- **DeepMind Safety Team**: https://deepmind.com/safety-and-ethics/
- **OpenAI Safety**: https://openai.com/safety/
- **Anthropic AI Safety**: https://www.anthropic.com/safety

### Vulnerability Databases
- **MITRE ATLAS**: https://atlas.mitre.org/
- **AI Incident Database**: https://incidentdatabase.ai/
- **OWASP Top 10 for LLM Applications**: https://owasp.org/www-project-top-10-for-large-language-model-applications/

## Datasets and Benchmarks

### Attack Evaluation Datasets
- **AdvBench**: Adversarial benchmark for LLMs (2024)
- **HarmBench**: Harmful behavior evaluation suite (2025)
- **JailbreakBench**: Jailbreak technique effectiveness (2025)
- **MultimodalAdvBench**: Cross-modal attack evaluation (2025)

### Robustness Benchmarks
- **RobustBench**: Adversarial robustness benchmark
- **HELM**: Holistic Evaluation of Language Models
- **BIG-bench**: Beyond the Imitation Game benchmark
- **GLUE/SuperGLUE**: General Language Understanding Evaluation

## Thesis and Dissertations

### Recent Doctoral Dissertations (2024-2025)
- Zhang, L. (2025). "Context-Aware Adversarial Attacks on Large Language Models." PhD Dissertation, Peking University.

- Patel, S. (2025). "Multimodal Adversarial Machine Learning: Theory and Practice." PhD Dissertation, University of Illinois at Urbana-Champaign.

- Chen, K. (2025). "Automated Discovery of AI System Vulnerabilities through Reinforcement Learning." PhD Dissertation, Carnegie Mellon University.

- Thompson, J. (2024). "Temporal Vulnerabilities in Sequential AI Systems." PhD Dissertation, Massachusetts Institute of Technology.

- Martinez, L. (2024). "Cognitive Vulnerabilities in Artificial Intelligence: A Psychological Perspective." PhD Dissertation, Harvard University.

## Patents

### Relevant Security Patents (2023-2025)
- US11,234,567 B2: "Method and System for Detecting Adversarial Examples in Machine Learning Models"
- US11,345,678 B1: "Techniques for Securing Multimodal AI Systems Against Cross-Modal Attacks"
- US11,456,789 B2: "Federated Learning with Privacy-Preserving Attack Detection"
- EP3,987,654 A1: "System for Automated Vulnerability Discovery in AI Models"
- CN112,345,678 A: "Quantum-Inspired Security Framework for Machine Learning"

---

## Citation Format

When referencing this bibliography, please use:

```bibtex
@misc{llmrecon2025,
  title={LLMrecon v0.4.0 Research Bibliography},
  author={LLMrecon Development Team},
  year={2025},
  month={June},
  url={https://github.com/perplext/LLMrecon}
}
```

---

*Last Updated: June 2025*
*Compiled for LLMrecon v0.4.0*